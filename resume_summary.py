# -*- coding: utf-8 -*-
"""resume_summary.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wsi0uzkwD0khY8nkDGaGC5m9wwxuR7_y
"""

# ! pip install spacy==2.0.13

import json
import random
import logging
from sklearn.metrics import classification_report
from sklearn.metrics import precision_recall_fscore_support
from spacy.gold import GoldParse
from spacy.scorer import Scorer
from sklearn.metrics import accuracy_score
import spacy
import re
import os
import random

import matplotlib.pyplot as plt
from collections import defaultdict
import pandas as pd

model_save_loc="/content/saved_model"
if not os.path.exists(model_save_loc):
  os.makedirs(model_save_loc)
train_epochs = 3
validation_test_set_size=10
test_data_loc ="/content/testdata.json"
train_data_loc ="/content/traindata.json"
eda_output_save_loc = '/content/eda_output'

# if not os.path.exists(eda_output_save_loc):
#   os.makedirs(eda_output_save_loc)

def get_annotatin_data(raw_line):
    in_line = defaultdict(list, **raw_line)
    if 'annotation' in in_line:
        labels = in_line['annotation']
        for c_lab in labels:
            if len(c_lab['label'])>0:
                in_line[c_lab['label'][0]] += c_lab['points']
    return in_line

# def eda_1(train_data_loc):
#   train_data = open(train_data_loc,'r')
#   all_json={}
#   c=0
#   for f_line in train_data.readlines():
#       json_=json.loads(f_line)
#       json_={str(c):get_annotatin_data(json_)}
#       all_json.update(json_)
#       c+=1
#   # print(all_json)
#   resume_df = pd.DataFrame(all_json)
#   resume_df = resume_df.T

#   print(len("annotated labels"),resume_df.columns)
#   return resume_df

f= open(train_data_loc,'r')
resume_data = [json.loads(f_line) for f_line in f.readlines()]
resume_df = pd.DataFrame([get_annotatin_data(line) for line in resume_data])
f.close()

# resume_df.head()

# df = eda_1(train_data_loc)

# resume_df.describe()

# ! pip install 
# import pandas_profiling
# pandas_profiling.ProfileReport(resume_df)

resume_df.isna().sum()



def trim_entity_spans(data: list) -> list:

    invalid_span_tokens = re.compile(r'\s')

    cleaned_data = []
    for text, annotations in data:
        entities = annotations['entities']
        valid_entities = []
        for start, end, label in entities:
            valid_start = start
            valid_end = end
            while valid_start < len(text) and invalid_span_tokens.match(
                    text[valid_start]):
                valid_start += 1
            while valid_end > 1 and invalid_span_tokens.match(
                    text[valid_end - 1]):
                valid_end -= 1
            valid_entities.append([valid_start, valid_end, label])
        cleaned_data.append([text, {'entities': valid_entities}])

    return cleaned_data

def json_to_spacy(JSON_FilePath):
    try:
        training_data = []
        lines=[]
        with open(JSON_FilePath, 'r') as f:
            lines = f.readlines()
        # lines= trim_entity_spans(file_lines)
        print(lines)
        for line in lines:
            data = json.loads(line)
            text = data['content']
            entities = []
            for annotation in data['annotation']:
                #only a single point in text annotation.
                point = annotation['points'][0]
                labels = annotation['label']
                # handle both list of labels or a single label.
                if not isinstance(labels, list):
                    labels = [labels]

                for label in labels:
                   
                    entities.append((point['start'], point['end'] + 1 ,label))


            training_data.append((text, {"entities" : entities}))

        return training_data
    except Exception as e:
        logging.exception("Unable to process " + dataturks_JSON_FilePath + "\n" + "error = " + str(e))
        return None

def validation_f_score(nlp):
  examples = json_to_spacy(test_data_loc)
  tp=0
  tr=0
  tf=0

  ta=0
  c=0     

  for text,annot in examples[:validation_test_set_size]:
      # print("entities:",annot.get("entities"))
      doc_to_test=nlp(text)
      d={}
      for ent in doc_to_test.ents:
          d[ent.label_]=[0,0,0,0,0,0]
      for ent in doc_to_test.ents:
          doc_gold_text= nlp.make_doc(text)
          gold = GoldParse(doc_gold_text, entities=annot.get("entities"))
          y_true = [ent.label_ if ent.label_ in x else 'Not '+ent.label_ for x in gold.ner]
          y_pred = [x.ent_type_ if x.ent_type_ ==ent.label_ else 'Not '+ent.label_ for x in doc_to_test]  
          if(d[ent.label_][0]==0):
              (p,r,f,s)= precision_recall_fscore_support(y_true,y_pred,average='weighted')
              a=accuracy_score(y_true,y_pred)
              d[ent.label_][0]=1
              d[ent.label_][1]+=p
              d[ent.label_][2]+=r
              d[ent.label_][3]+=f
              d[ent.label_][4]+=a
              d[ent.label_][5]+=1
      c+=1

  average_f_score = 0
  for i in d:
      # print("F-score : "+str(d[i][3]/d[i][5]))
      average_f_score+=d[i][3]/d[i][5]

  return average_f_score/len(d)

def train_model():

    TRAIN_DATA = json_to_spacy(train_data_loc)
    nlp = spacy.blank('en')  # create blank Language class
  
    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last=True)
       

    # add labels
    for _, annotations in TRAIN_DATA:
         for ent in annotations.get('entities'):
            ner.add_label(ent[2])

    # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    current_set_best_results=0
    with nlp.disable_pipes(*other_pipes):  # only train NER
        optimizer = nlp.begin_training()
        for itn in range(train_epochs):
            random.shuffle(TRAIN_DATA)
            losses = {}
            for text, annotations in TRAIN_DATA:
                nlp.update(
                    [text],  # batch of texts
                    [annotations],  # batch of annotations
                    drop=0.3,  # dropout - make it harder to memorise data
                    sgd=optimizer,  # callable to update weights
                    losses=losses)
            print(losses)


            

            val_f_score = validation_f_score(nlp)

            if val_f_score >= current_set_best_results:
                print("found a better ner pipeline!")
                best_ner = nlp.get_pipe('ner')

                current_set_best_results = val_f_score
                # print("current_set_best_results:",current_set_best_results)
    
    nlp.replace_pipe('ner', best_ner)
    nlp.to_disk(model_save_loc)

# train_model()

def run_model_on_test_set():
  nlp = spacy.load(model_save_loc)
  examples = json_to_spacy("/content/testdata.json")
  tp=0
  tr=0
  tf=0

  ta=0
  c=0        
  for text,annot in examples[validation_test_set_size:]:
      # print("entities:",annot.get("entities"))
      doc_to_test=nlp(text)
      d={}
      for ent in doc_to_test.ents:
          d[ent.label_]=[0,0,0,0,0,0]
      for ent in doc_to_test.ents:
          doc_gold_text= nlp.make_doc(text)
          gold = GoldParse(doc_gold_text, entities=annot.get("entities"))
          y_true = [ent.label_ if ent.label_ in x else 'Not '+ent.label_ for x in gold.ner]
          y_pred = [x.ent_type_ if x.ent_type_ ==ent.label_ else 'Not '+ent.label_ for x in doc_to_test]  
          if(d[ent.label_][0]==0):
              (p,r,f,s)= precision_recall_fscore_support(y_true,y_pred,average='weighted')
              a=accuracy_score(y_true,y_pred)
              d[ent.label_][0]=1
              d[ent.label_][1]+=p
              d[ent.label_][2]+=r
              d[ent.label_][3]+=f
              d[ent.label_][4]+=a
              d[ent.label_][5]+=1
      c+=1
  for i in d:
      print("\n For Entity "+i+"\n")
      print("Accuracy : "+str((d[i][4]/d[i][5])*100)+"%")
      print("Precision : "+str(d[i][1]/d[i][5]))
      print("Recall : "+str(d[i][2]/d[i][5]))
      print("F-score : "+str(d[i][3]/d[i][5]))

# run_nlp_model()

# ! pip install PyPDF2

import PyPDF2

def test_pdf():
  pdf_file = open('/content/test_pdf.pdf', mode='rb')
  text=""
  read_pdf = PyPDF2.PdfFileReader(pdf_file)
  number_of_pages = read_pdf.getNumPages()
  print(number_of_pages)
  for page_number in range(number_of_pages):  
      page = read_pdf.getPage(page_number)
      page_content = page.extractText()
      # print("page_content:",page_content)
      text+=page_content
  nlp = spacy.load(model_save_loc)
  doc_gold_text= nlp(text)

  for ent in doc_gold_text.ents:
      print(ent.label_,":",ent.text )

# train_model()
# run_model_on_test_set()
test_pdf()

# ! sudo apt-get install build-essential libpoppler-cpp-dev pkg-config python-dev
# ! pip install pdftotext

# import pdftotext
# import io

# with open("/content/Resume (1).pdf", "r") as f:
#     # f=io.BytesIO(f)
#     pdf = pdftotext.PDF(f)
# print(pdf.read_all())

# !zip -r /content/saved_model.zip /content/saved_model

# from google.colab import files
# files.download("/content/saved_model.zip")

